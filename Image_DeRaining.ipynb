{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image DeRaining.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKJLtIOdz3Jt"
      },
      "source": [
        "#Loading Dataset and initializing Wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vxtKR6iocGE"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import os\n",
        "import os.path\n",
        "import h5py\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchsummary import summary\n",
        "from skimage.measure.simple_metrics import compare_psnr\n",
        "import random\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_EBa_yHo2Nr"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU41WHfz8yON"
      },
      "source": [
        "!pip install wandb --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiCpVBP46bie"
      },
      "source": [
        "import wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGY1Z3NE6cHd"
      },
      "source": [
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtlwNmHT6eEc"
      },
      "source": [
        "wandb.init(project=\"Deraining\",name = \"SSIM_100L_PRN\",resume = True,id=\"21n7u9u9\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjbJMwcL1wLZ"
      },
      "source": [
        "def normalize(data):\n",
        "    return data / 255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXlcLOWhK66f"
      },
      "source": [
        "def Im2Patch(img, win, stride=1):\n",
        "    k = 0\n",
        "    endc = img.shape[0]\n",
        "    endw = img.shape[1]\n",
        "    endh = img.shape[2]\n",
        "    patch = img[:, 0:endw - win + 0 + 1:stride, 0:endh - win + 0 + 1:stride]\n",
        "    TotalPatNum = patch.shape[1] * patch.shape[2]\n",
        "    Y = np.zeros([endc, win * win, TotalPatNum], np.float32)\n",
        "\n",
        "    for i in range(win):\n",
        "        for j in range(win):\n",
        "            patch = img[:, i:endw - win + i + 1:stride, j:endh - win + j + 1:stride]\n",
        "            Y[:, k, :] = np.array(patch[:]).reshape(endc, TotalPatNum)\n",
        "            k = k + 1\n",
        "    return Y.reshape([endc, win, win, TotalPatNum])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqIj6huD8sAW"
      },
      "source": [
        "patch_size=100\n",
        "stride=80"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TGBn_WPvici"
      },
      "source": [
        "#Preparing data rain trainL\n",
        "list_x = []\n",
        "list_y = []\n",
        "patch_size=100\n",
        "stride=80\n",
        "\n",
        "input_path = os.path.join(\"/content/drive/MyDrive/Rain_Dataset/Rain_100L_train\")\n",
        "target_path = os.path.join(\"/content/drive/MyDrive/Rain_Dataset/DeRain_100L_train\")\n",
        "\n",
        "\n",
        "train_num = 0\n",
        "\n",
        "for i in range(200):\n",
        "    target_file = \"norain-%d.png\" % (i + 1)\n",
        "    target = cv2.imread(os.path.join(target_path,target_file))\n",
        "    b, g, r = cv2.split(target)\n",
        "    target = cv2.merge([r, g, b])\n",
        "    input_file = \"rain-%d.png\" % (i + 1)\n",
        "    input = cv2.imread(os.path.join(input_path,input_file))\n",
        "    b, g, r = cv2.split(input)\n",
        "    input = cv2.merge([r, g, b])\n",
        "\n",
        "    #plt.imshow(input)\n",
        "    #plt.imshow(target)\n",
        "\n",
        "    target = np.float32(normalize(target))\n",
        "    target_patches = Im2Patch(target.transpose(2,0,1), win=patch_size, stride=stride)\n",
        "    for n in range(target_patches.shape[3]):\n",
        "        target1 = target_patches[:,:,:,n]\n",
        "        list_x.append(target1)\n",
        "    #plt.imshow(target1.transpose(1,2,0))\n",
        "    #plt.imshow(target_patches[:,:,:,0].transpose(1,2,0))\n",
        "\n",
        "    input = np.float32(normalize(input))\n",
        "    input_patches = Im2Patch(input.transpose(2, 0, 1), win=patch_size, stride=stride)\n",
        "    for n in range(input_patches.shape[3]):\n",
        "        input1 = input_patches[:,:,:,n]\n",
        "        list_y.append(input1)\n",
        "\n",
        "    #print(\"target file: %s # samples: %d\" % (input_file, target_patches.shape[3]))\n",
        "#target_h5f.close()\n",
        "#input_h5f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDdij__I-umL"
      },
      "source": [
        "train_num = 0\n",
        "\n",
        "for i in range(200):\n",
        "    target_file = \"norain-%d.png\" % (i + 1)\n",
        "    target = cv2.imread(os.path.join(target_path,target_file))\n",
        "    b, g, r = cv2.split(target)\n",
        "    target = cv2.merge([r, g, b])\n",
        "    input_file = \"rain-%d.png\" % (i + 1)\n",
        "    input = cv2.imread(os.path.join(input_path,input_file))\n",
        "    b, g, r = cv2.split(input)\n",
        "    input = cv2.merge([r, g, b])\n",
        "\n",
        "    #plt.imshow(input)\n",
        "    #plt.imshow(target)\n",
        "    target = cv2.flip(target, 1)\n",
        "    input = cv2.flip(input, 1)\n",
        "\n",
        "    target = np.float32(normalize(target))\n",
        "    target_patches = Im2Patch(target.transpose(2,0,1), win=patch_size, stride=stride)\n",
        "    for n in range(target_patches.shape[3]):\n",
        "        target1 = target_patches[:,:,:,n]\n",
        "        list_x.append(target1)\n",
        "    #plt.imshow(target1.transpose(1,2,0))\n",
        "    #plt.imshow(target_patches[:,:,:,0].transpose(1,2,0))\n",
        "\n",
        "    input = np.float32(normalize(input))\n",
        "    input_patches = Im2Patch(input.transpose(2, 0, 1), win=patch_size, stride=stride)\n",
        "    for n in range(input_patches.shape[3]):\n",
        "        input1 = input_patches[:,:,:,n]\n",
        "        list_y.append(input1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWOvUm9aEM8t"
      },
      "source": [
        "%cd /content/drive/My Drive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRQudiMcCep2"
      },
      "source": [
        "np.save(\"train_100L_Rain.npy\",list_y)\n",
        "np.save(\"train_100L_Derain.npy\",list_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOxB2UqCLJJD"
      },
      "source": [
        "image_x = np.load(\"/content/drive/MyDrive/train_100L_Rain.npy\")\n",
        "image_y = np.load(\"/content/drive/MyDrive/train_100L_Derain.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i662sFA0bed"
      },
      "source": [
        "#Model and Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TI8KmF6Pfcf"
      },
      "source": [
        "trans = transforms.Compose([transforms.ToTensor()])\n",
        "class Custom_Dataset(Dataset):\n",
        "  def __init__(self,X,Y):\n",
        "    self.images=X\n",
        "    self.labels=Y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self,i):\n",
        "    x = self.images[i]\n",
        "    y = self.labels[i]\n",
        "    x = trans(x)\n",
        "   # x = x.permute(1,2,0)\n",
        "    y = trans(y)\n",
        "   # y = y.permute(1,2,0)\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzKv71SdPnSl"
      },
      "source": [
        "class PRN(nn.Module):\n",
        "    def __init__(self, recurrent_iter=6, use_GPU=True):\n",
        "        super(PRN, self).__init__()\n",
        "        self.iteration = recurrent_iter\n",
        "\n",
        "        self.conv0 = nn.Sequential(\n",
        "            nn.Conv2d(6, 32, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.res_conv1 = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.res_conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.res_conv3 = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.res_conv4 = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.res_conv5 = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(32, 3, 3, 1, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        x = input\n",
        "\n",
        "        x_list = []\n",
        "        for i in range(self.iteration):\n",
        "            x = torch.cat((input, x), 1)\n",
        "            x = self.conv0(x)\n",
        "            resx = x\n",
        "            x = F.relu(self.res_conv1(x) + resx)\n",
        "            resx = x\n",
        "            x = F.relu(self.res_conv2(x) + resx)\n",
        "            resx = x\n",
        "            x = F.relu(self.res_conv3(x) + resx)\n",
        "            resx = x\n",
        "            x = F.relu(self.res_conv4(x) + resx)\n",
        "            resx = x\n",
        "            x = F.relu(self.res_conv5(x) + resx)\n",
        "            x = self.conv(x)\n",
        "\n",
        "            x = x + input\n",
        "            x_list.append(x)\n",
        "\n",
        "        return x, x_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrK2cwK4XuY4"
      },
      "source": [
        "model = PRN(recurrent_iter=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6FgYuAQzgMC"
      },
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqSrXLhXzYQU"
      },
      "source": [
        "device = get_default_device()\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jGkBfjwzi4h"
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCcoOZjm7dHK"
      },
      "source": [
        "def batch_PSNR(img, imclean, data_range):\n",
        "    Img = img.data.cpu().numpy().astype(np.float32)\n",
        "    Iclean = imclean.data.cpu().numpy().astype(np.float32)\n",
        "    PSNR = 0\n",
        "    for i in range(Img.shape[0]):\n",
        "        PSNR += compare_psnr(Iclean[i,:,:,:], Img[i,:,:,:], data_range=data_range)\n",
        "    return (PSNR/Img.shape[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7C-QoBjJZIq"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from math import exp\n",
        " \n",
        "def gaussian(window_size, sigma):\n",
        "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
        "    return gauss/gauss.sum()\n",
        "\n",
        "def create_window(window_size, channel):\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
        "    return window\n",
        "\n",
        "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
        "    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n",
        "    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1*mu2\n",
        "\n",
        "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n",
        "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n",
        "\n",
        "    C1 = 0.01**2\n",
        "    C2 = 0.03**2\n",
        "\n",
        "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
        "\n",
        "    if size_average:\n",
        "        return ssim_map.mean()\n",
        "    else:\n",
        "        return ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "class SSIM(torch.nn.Module):\n",
        "    def __init__(self, window_size = 11, size_average = True):\n",
        "        super(SSIM, self).__init__()\n",
        "        self.window_size = window_size\n",
        "        self.size_average = size_average\n",
        "        self.channel = 1\n",
        "        self.window = create_window(window_size, self.channel)\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        (_, channel, _, _) = img1.size()\n",
        "\n",
        "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
        "            window = self.window\n",
        "        else:\n",
        "            window = create_window(self.window_size, channel)\n",
        "            \n",
        "            if img1.is_cuda:\n",
        "                window = window.cuda(img1.get_device())\n",
        "            window = window.type_as(img1)\n",
        "            \n",
        "            self.window = window\n",
        "            self.channel = channel\n",
        "\n",
        "\n",
        "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
        "\n",
        "def ssim(img1, img2, window_size = 11, size_average = True):\n",
        "    (_, channel, _, _) = img1.size()\n",
        "    window = create_window(window_size, channel)\n",
        "    \n",
        "    if img1.is_cuda:\n",
        "        window = window.cuda(img1.get_device())\n",
        "    window = window.type_as(img1)\n",
        "    \n",
        "    return _ssim(img1, img2, window, window_size, channel, size_average)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PTtkfO_u0Nb"
      },
      "source": [
        "def fit(epochs=epochs,lr=lr,loader_train = loader_train):\n",
        "\n",
        "    criterion = SSIM()\n",
        "    #criterion = nn.MSELoss(size_average=False)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    #scheduler = MultiStepLR(optimizer, milestones=milestone, gamma=0.2)\n",
        "\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        cum_loss = 0\n",
        "        cum_psnr = 0\n",
        "        print(\"*******************************************************\")\n",
        "        for i, (input_train, target_train) in enumerate(loader_train, 0):\n",
        "            model.zero_grad()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_train, target_train = Variable(input_train), Variable(target_train)\n",
        "\n",
        "            input_train, target_train = input_train.to(device), target_train.to(device)\n",
        "\n",
        "            out_train, _ = model(input_train)\n",
        "            pixel_metric = criterion(target_train, out_train)\n",
        "            loss = -pixel_metric\n",
        "            cum_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "   \n",
        "            out_train = torch.clamp(out_train, 0., 1.)\n",
        "            psnr_train = batch_PSNR(out_train, target_train, 1.)\n",
        "\n",
        "            cum_psnr = cum_psnr + psnr_train\n",
        "\n",
        "            #print(\"Loss after iteration \",i,\" :\",loss.item())\n",
        "            #print(\"PSNR after iteration \",i,\" :\",psnr_train)\n",
        "\n",
        "            wandb.log({'Loss_Train_batch':loss,'PSNR_batch':psnr_train})\n",
        "        print(\"*******************************************************\")\n",
        "        print(\"Loss after epoch \",epoch,\" :\",cum_loss/len(loader_train))\n",
        "        print(\"PSNR after epoch \",epoch,\" :\",cum_psnr/len(loader_train))\n",
        "        print(\"*******************************************************\")\n",
        "        print(\"*******************************************************\")\n",
        "        wandb.log({'Loss_Train':cum_loss/len(loader_train),'PSNR':cum_psnr/len(loader_train)})\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/derain100L_pnr_ssimloss_try1.pth\")\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sQQtLsx0h2l"
      },
      "source": [
        "#Training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKizTSkWAhJD"
      },
      "source": [
        "lr = 0.0001\n",
        "epochs = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGLgAHnBFMjv"
      },
      "source": [
        "dataset_train = Custom_Dataset(X = image_x,Y=image_y)\n",
        "loader_train = DataLoader(dataset=dataset_train,num_workers=2, batch_size=18,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1aXOfOrXMLN"
      },
      "source": [
        "list_x = []\n",
        "list_y = []\n",
        "\n",
        "input_path = os.path.join(\"/content/drive/MyDrive/Rain_Dataset/Rain_100L\")\n",
        "target_path = os.path.join(\"/content/drive/MyDrive/Rain_Dataset/DeRain_100L\")\n",
        "\n",
        "\n",
        "\n",
        "for i in range(100):\n",
        "    if (i<9):\n",
        "      target_file = \"norain-00%d.png\" % (i+1)\n",
        "      input = cv2.imread(os.path.join(target_path,target_file))\n",
        "\n",
        "      input_file = \"rain-00%d.png\" % (i+1)\n",
        "      target = cv2.imread(os.path.join(input_path,input_file)) \n",
        "\n",
        "    elif i<99:\n",
        "      target_file = \"norain-0%d.png\" % (i+1)\n",
        "      input = cv2.imread(os.path.join(target_path,target_file))\n",
        "\n",
        "      input_file = \"rain-0%d.png\" % (i+1)\n",
        "      target = cv2.imread(os.path.join(input_path,input_file)) \n",
        "\n",
        "    else :\n",
        "      target_file = \"norain-%d.png\" % (i+1)\n",
        "      input = cv2.imread(os.path.join(target_path,target_file))\n",
        "\n",
        "      input_file = \"rain-%d.png\" % (i+1)\n",
        "      target = cv2.imread(os.path.join(input_path,input_file)) \n",
        "\n",
        "\n",
        "    \n",
        "    b, g, r = cv2.split(target)\n",
        "    target = cv2.merge([r, g, b])\n",
        "   \n",
        "   \n",
        "    b, g, r = cv2.split(input)\n",
        "    input = cv2.merge([r, g, b]) \n",
        "      \n",
        "    target = np.float32(normalize(target))\n",
        "    input  = np.float32(normalize(input))\n",
        "    \n",
        "\n",
        "    list_x.append(input)\n",
        "    list_y.append(target)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adoQ7zvZdCLI"
      },
      "source": [
        "np.save(\"test_100L_Rain.npy\",list_y)\n",
        "np.save(\"test_100L_Derain.npy\",list_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77FvI4Ipdop0"
      },
      "source": [
        "dataset_test = Custom_Dataset(X = list_y,Y=list_x)\n",
        "loader_test = DataLoader(dataset=dataset_test,num_workers=2, batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOLjnhd_ehLs"
      },
      "source": [
        "cum_loss  = 0\n",
        "cum_psnr  = 0\n",
        "criterion = SSIM()\n",
        "for (input_test, target_test) in loader_test:\n",
        "            input_test, target_test = Variable(input_test), Variable(target_test)\n",
        "\n",
        "            input_test, target_test = input_test.to(device), target_test.to(device)\n",
        "\n",
        "            out_test, _ = model(input_test)\n",
        "\n",
        "            out_test = torch.clamp(out_test, 0., 1.)\n",
        "            psnr_test = batch_PSNR(out_test, target_test, 1)\n",
        "            pixel_metric = criterion(target_test, out_test)\n",
        "            \n",
        "            cum_loss += pixel_metric.item()\n",
        "            cum_psnr = cum_psnr + psnr_test\n",
        "print(\"The ssim of the testbset is: \",cum_loss/len(loader_test))\n",
        "print(\"The psnr of the testbset is: \",cum_psnr/len(loader_test))\n",
        "wandb.log({'SSIM_TEST':cum_loss/len(loader_test),'PSNR_TEST':cum_psnr/len(loader_test)})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPKHeaZUJln-"
      },
      "source": [
        "fit(epochs=300,lr=lr,loader_train = loader_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkGvKYV8i6G5"
      },
      "source": [
        "for i in range(98):\n",
        "\n",
        "    input_path = os.path.join(\"/content/drive/MyDrive/Rain_Dataset/Rain_100L\")\n",
        "    target_path = os.path.join(\"/content/drive/MyDrive/Rain_Dataset/DeRain_100L\")\n",
        "    if (i<9):\n",
        "      target_file = \"norain-00%d.png\" % (i+1)\n",
        "      inp = cv2.imread(os.path.join(target_path,target_file))\n",
        "\n",
        "      input_file = \"rain-00%d.png\" % (i+1)\n",
        "      y = cv2.imread(os.path.join(input_path,input_file))   \n",
        "    \n",
        "    else:\n",
        "      target_file = \"norain-0%d.png\" % (i+1)\n",
        "      inp = cv2.imread(os.path.join(target_path,target_file))\n",
        "\n",
        "      input_file = \"rain-0%d.png\" % (i+1)\n",
        "      y = cv2.imread(os.path.join(input_path,input_file))     \n",
        "    \n",
        "    true_y = y\n",
        "    b, g, r = cv2.split(y)\n",
        "    y = cv2.merge([r, g, b])\n",
        "    y = normalize(np.float32(y))\n",
        "    y = np.expand_dims(y.transpose(2, 0, 1), 0)\n",
        "    y = torch.Tensor(y)\n",
        "    y = y.to(device)\n",
        "    out, _ = model(y)\n",
        "    save_out = np.uint8(255 * out.data.cpu().numpy().squeeze())  \n",
        "    save_out = save_out.transpose(1, 2, 0)\n",
        "    b, g, r = cv2.split(save_out)\n",
        "    save_out = cv2.merge([r, g, b])\n",
        "    \n",
        "\n",
        "    wandb.log({\n",
        "           \"rained_test\": wandb.Image(true_y),\n",
        "           \"true_derained_test\": wandb.Image(inp),\n",
        "           \"predicted_derained_test\": wandb.Image(save_out)\n",
        "    })"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}